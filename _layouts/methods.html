---
layout: base
---

<div class="usa-section background-brand-dark">
  <div class="usa-grid">
    <h1>{{ page.title }}</h1>
  </div>
</div>
<div class="usa-grid usa-content main-content" id="main">
  {% if page.summary %}
    <p class="billboard-message">{{ page.summary }}</p>
  {% endif %}
  {% if page.image and page.image_full %}
    <img src="{{ page.image | prepend: site.baseurl }}" alt="{{ page.image_alt_text }}">
  {% elsif page.image %}
    <div class="page--banner" style="background-image: url({{ page.image | prepend: site.baseurl }});" role="img" {% if page.image_alt_text %} aria-labelledby="caption"{% endif %}>
    {% if page.image_alt_text %}
      <p class="usa-sr-only" id="caption">{{ page.image_alt_text }}</p>
    {% endif %}
    </div>
  {% endif %}
  {{ content }}
</div>
 <div class="usa-grid">
    <div class="usa-width-one-fourth">
    <center><img src="{{ '/assets/img/icons/1.png' | prepend: site.baseurl }}" height="100" width="100"></center>
    </div>
    <div class="usa-width-three-fourths">
      <h3>Step 1: Partner with Federal Agencies to target priority outcomes</h3>
      <p>Agencies maintain priorities through their Congressional Justifications, Annual Performance Plans, Strategic Plans, Agency Priority Goals, Cross-Agency Priority Goals, Learning Agendas, and many other planning efforts. In conversations with collaborators, we discuss the most important questions that need to be answered in order to improve program implementation and performance, and define a meaningful outcome at the start that we hope to move the needle on. Each project is then vetted for feasibility, proper planning, and potential impact for stakeholders in a Federal program or policy.</p>
      <p><a href="{{ '/assets/files/ProjectInitiation.pdf' | prepend: site.baseurl }}">Project Initiation Document</a></p>
    </div>
  </div>
</br></br>
 <div class="usa-grid">
    <div class="usa-width-one-fourth">
    <center><img src="{{ '/assets/img/icons/2.png' | prepend: site.baseurl }}" height="100" width="100"></center>
    </div>
    <div class="usa-width-three-fourths">
      <h3>Step 2: Translate evidence-based insights into concrete recommendations</h3>
      <p>Our collaborators, who are civil servants with years of experience working to deliver programs across the government, are the experts on how their programs work and often have the best ideas for how to improve them. OES team members support their efforts by bringing diverse backgrounds in the social sciences and an “evidence base” from their area of study to more deeply understand program bottlenecks, offering recommendations for improving government that have demonstrated success in peer-reviewed contexts using quantitative methods.</p>
      <p><a href="{{ '/assets/files/ProjectDesign.pdf' | prepend: site.baseurl }}">Project Design Document</a></p>
    </div>
  </div>
</br></br>
 <div class="usa-grid">
    <div class="usa-width-one-fourth">
    <center><img src="{{ '/assets/img/icons/3.png' | prepend: site.baseurl }}" height="100" width="100"></center>
    </div>
    <div class="usa-width-three-fourths">
      <h3>Step 3: Embed tests using randomized evaluations</h3>
        <p>"Tests" to OES are opportunities to truly learn what works - and are grounded in scientific methods.</p> 
            <p>- Whenever possible, we aim to <a href="https://oes.gsa.gov/methodsdeepdive">randomly assign</a> individuals or groups to a treatment condition. This is what enables us to conclude that improvements in outcomes were actually caused by the program change(s) that we tested.</p> 
            <p>- In designing tests, we give particular attention to <a href="https://oes.gsa.gov/methodsdeepdive">statistical power</a>. Briefly, statistical power is a study’s ability to correctly detect whether a program improvement was effective (assuming that it was indeed effective).</p> 
            <p>- Finally, one of the most important steps we take is committing to a detailed <a href="https://oes.gsa.gov/methodsdeepdive">analysis plan</a> before we begin working with data. As the recent replication crisis in the social sciences has shown, if scientists allow themselves too much flexibility in analyzing data they may inadvertently detect effects that are the result of “fishing” or “p-hacking.”</p> 
        <p><a href="{{ '/assets/files/AnalysisPlan.pdf' | prepend: site.baseurl }}">Analysis Plan Document</a></p>
   </div>
  </div>
</br></br>
 <div class="usa-grid">
    <div class="usa-width-one-fourth">
    <center><img src="{{ '/assets/img/icons/4.png' | prepend: site.baseurl }}" height="100" width="100"></center>
    </div>
    <div class="usa-width-three-fourths">
      <h3>Step 4: Analyze results utilizing exisitng administrative data</h3>
      <p>As OES seeks to embed tests into ongoing operations, new data collections are generally not an option as they can be costly, time-intensive, and can require extensive approval processes. However, administrative data collected by government entities for program administration, regulatory, or law enforcement purposes offer rich information that is often underutilized. <a href="https://obamawhitehouse.archives.gov/sites/default/files/omb/memoranda/2014/m-14-06.pdf">Office of Management and Budget (OMB) Memorandum M-14-06 Guidance for Providing and Using Administrative Data for Statistical Purposes</a> promotes increased agency use of administrative data for evidence-building. OES team members work with agency partners to leverage existing data to measure the effect of a program change on a the priority outcome of interest.
</p>
     <p><a href="{{ '/assets/files/RecordAnalysis.pdf' | prepend: site.baseurl }}">Record of Analysis Document</a></p>
    </div>
  </div>
</br></br>
 <div class="usa-grid">
    <div class="usa-width-one-fourth">
    <center><img src="{{ '/assets/img/icons/5.png' | prepend: site.baseurl }}" height="100" width="100"></center>
    </div>
    <div class="usa-width-three-fourths">
      <h3>Step 5: Ensure our work meets evaluation best practice</h3>
      <p>In keeping with our team’s commitment to reproducibility, before we finalize an analysis, we submit it to an internal replication that we call Reanalysis. This is done by asking an independent reanalyst — an analyst who does not know the results of the initial analysis  — to write new code to analyze the administrative data and generate results that address the study’s research objectives. Reanalysis serves as a check on (1) the computer code that the first analyst used to analyze the data, (2) any exploratory analyses that might have been conducted, and (3) any departures from the Analysis Plan that might have been necessary due to unanticipated features of the data.
The reanalyst’s goal is to replicate the initial analysis from scratch, working only from the raw data and the Analysis Plan. It is important that the reanalyst not know the results of the initial analysis. Because we generally hold a team discussion of the initial analysis before Reanalysis occurs, we make sure that the reanalyst is recused from this discussion. Only when the reanalyst has finished do they look at the initial analyst’s write-up of results and findings.</p>
     <p><a href="{{ '/assets/files/RecordReanalysis.pdf' | prepend: site.baseurl }}">Record of Reanalysis Document</a></p>
    </div>
  </div>
</br></br>
 <div class="usa-grid">
    <div class="usa-width-one-fourth">
    <center><img src="{{ '/assets/img/icons/6.png' | prepend: site.baseurl }}" height="100" width="100"></center>
    </div>
    <div class="usa-width-three-fourths">
      <h3>Step 6: Measure impact and build evidence to continuously improve</h3>
      <p>As part of our commitment to transparency and learning, OES shares all findings from every completed evaluation. This helps ensure federal collaborators can learn what works and what doesn’t, and also learn from each other’s experiences. Results which are surprising or run counter to our expectations are just as important to share and often offer valuable lessons. While we do aim to submit our work to peer-review publications, our first priority is producing materials that enable decision makers to quickly digest results and understand their implications. We designed a number of project summary documents - from high-level summaries, to one-page abstracts, to template presentations, that our agency collaborators can use to circulate among their program teams, peers, and leadership.
</p>
     <p><a href="{{ '/assets/files/Abstract.pdf' | prepend: site.baseurl }}">Abstract Document</a></p>
    </div>
  </div>
